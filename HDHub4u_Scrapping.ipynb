{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## If you liked this Project then you can find more at https://pankajshah.netlify.app\n",
        "#### Feel free to contact me if you have any suggestions"
      ],
      "metadata": {
        "id": "VSkowOjJl0-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "wpw7owdll-da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "d = {'Title': [], 'Link': []}\n",
        "\n",
        "pages = int(input('Enter Pages:'))\n",
        "for i in tqdm(range(1, pages+1), desc=\"Pages\", unit=\"page\"):\n",
        "    url = f'https://www.hdhub4u.tw/page{i}'\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to retrieve page {i}. Skipping...\")\n",
        "        continue\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    recent_movies = soup.find('ul', class_='recent-movies')\n",
        "\n",
        "    if recent_movies:\n",
        "        for link in recent_movies.find_all('a', href=True):\n",
        "            title = link.get_text(strip=True)\n",
        "            href = link['href']\n",
        "\n",
        "            if title and href and href not in d['Link']:\n",
        "                d['Title'].append(title)\n",
        "                d['Link'].append(href)\n",
        "    else:\n",
        "        print(f\"Page {i}: The <ul> with class 'recent-movies' was not found.\")\n",
        "\n",
        "d_count_ofLinks = len(d['Link'])\n",
        "print(f'Found {d_count_ofLinks} movies in {pages} pages')\n",
        "print()\n",
        "final_data = {}\n",
        "\n",
        "i = 0\n",
        "\n",
        "with tqdm(total=d_count_ofLinks, desc=\"Processing Links\", unit=\"link\") as pbar:\n",
        "    while True:\n",
        "        if i >= d_count_ofLinks:\n",
        "            break\n",
        "\n",
        "        next_link = d['Link'][i]\n",
        "\n",
        "        try:\n",
        "            new_response = requests.get(next_link)\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Request failed for link {i+1}: {e}\")\n",
        "            i += 1\n",
        "            pbar.update(1)\n",
        "            continue\n",
        "\n",
        "        if new_response.status_code != 200:\n",
        "            print(f\"Failed to retrieve page {i+1}. Skipping...\")\n",
        "            i += 1\n",
        "            pbar.update(1)\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(new_response.content, 'html.parser')\n",
        "\n",
        "        kp_header_div = soup.find('div', class_='Z1hOCe')\n",
        "        first_h2_text = None\n",
        "        if kp_header_div:\n",
        "            first_h2 = kp_header_div.find('h2')\n",
        "            if first_h2:\n",
        "                first_h2_text = first_h2.get_text(strip=True)\n",
        "        else:\n",
        "            print(\"We Found Some Error (FIX IT)\")\n",
        "\n",
        "        link_element = soup.find(\n",
        "            'div',\n",
        "            style='text-align: center;',\n",
        "            attrs={'data-ved': '2ahUKEwi0gOTl-ozlAhWfILcAHVY0DbIQyxMoADAvegQIERAJ'}\n",
        "        )\n",
        "\n",
        "        if link_element:\n",
        "            links = link_element.find_all('a')\n",
        "            link_data = []\n",
        "\n",
        "            for link in links:\n",
        "                href = link.get('href')\n",
        "                text = link.get_text(strip=True)\n",
        "                link_data.append((text, href))\n",
        "\n",
        "            if first_h2_text:\n",
        "                final_data[first_h2_text] = link_data\n",
        "\n",
        "        i += 1\n",
        "        pbar.update(1)\n",
        "\n",
        "data_for_df = []\n",
        "\n",
        "for h2_text, links in final_data.items():\n",
        "    for link_text, link_url in links:\n",
        "        data_for_df.append({\n",
        "            'Movie Name': h2_text,\n",
        "            'Quality': link_text,\n",
        "            'URL': link_url\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(data_for_df)\n",
        "df.to_csv('Movies_Link.csv', index=False)\n",
        "print()\n",
        "print('Link Files Saved')\n",
        "files.download('Movies_Link.csv')\n"
      ],
      "metadata": {
        "id": "sfHIxQjql2R7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}