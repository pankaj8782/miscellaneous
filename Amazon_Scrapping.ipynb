{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install tqdm\n",
        "!pip install beautifulsoup4\n",
        "!pip install pandas\n",
        "!pip install random"
      ],
      "metadata": {
        "id": "At_wS9v-nQ96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfAfkL7Ym85q",
        "outputId": "a9a0eade-c583-42c3-a943-efbdebe4bc30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter what you want to search (e.g., laptop, phone): clothes\n",
            "Enter the number of pages to scrape: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rScrappinng:   0%|          | 0/1 [00:00<?, ?page/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error fetching page 1: 503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scrappinng: 100%|██████████| 1/1 [00:04<00:00,  4.37s/page]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to amazon_products.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "\n",
        "def get_product_details(search, page_number):\n",
        "    url = f'https://www.amazon.in/s?k={search}&page={page_number}'\n",
        "\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print()\n",
        "        print(f\"Error fetching page {page_number}: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    products = []\n",
        "\n",
        "    for item in soup.find_all('div', class_='sg-col-inner'):\n",
        "        product = {}\n",
        "\n",
        "        name_tag = item.find('a', class_='a-link-normal s-line-clamp-2 s-link-style a-text-normal')\n",
        "        if name_tag:\n",
        "            product['Product_Name'] = name_tag.get_text(strip=True)\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        mrp_tag = item.find('span', class_='a-price a-text-price')\n",
        "        if mrp_tag:\n",
        "            mrp_span = mrp_tag.find('span', class_='a-offscreen')\n",
        "            if mrp_span:\n",
        "                product['MRP'] = mrp_span.get_text(strip=True).replace('₹', '').replace(',', '')\n",
        "            else:\n",
        "                product['MRP'] = 'N/A'\n",
        "        else:\n",
        "            product['MRP'] = 'N/A'\n",
        "\n",
        "        discount_tag = item.find('span', class_='a-letter-space')\n",
        "        if discount_tag:\n",
        "            discount_span = discount_tag.find_next('span')\n",
        "            if discount_span:\n",
        "                product['Discount'] = discount_span.get_text(strip=True)\n",
        "            else:\n",
        "                product['Discount'] = 'N/A'\n",
        "        else:\n",
        "            product['Discount'] = 'N/A'\n",
        "\n",
        "        price_tag = item.find('span', class_='a-price-whole')\n",
        "        if price_tag:\n",
        "            product['Price'] = price_tag.get_text(strip=True).replace('₹', '').replace(',', '')\n",
        "        else:\n",
        "            product['Price'] = 'N/A'\n",
        "\n",
        "        rating_tag = item.find('span', class_='a-icon-alt')\n",
        "        if rating_tag:\n",
        "            product['Ratings'] = rating_tag.get_text(strip=True)\n",
        "        else:\n",
        "            product['Ratings'] = 'N/A'\n",
        "\n",
        "        products.append(product)\n",
        "\n",
        "    return products\n",
        "\n",
        "def scrape_amazon(search, pages_to_scrape):\n",
        "    all_products = []\n",
        "\n",
        "    for page in tqdm(range(1, pages_to_scrape + 1), desc=\"Scrappinng\", unit=\"page\"):\n",
        "        products = get_product_details(search, page)\n",
        "        all_products.extend(products)\n",
        "\n",
        "        # Random delay between 2 to 5 seconds to avoid detection\n",
        "        time.sleep(random.uniform(2, 5))\n",
        "\n",
        "    if all_products:\n",
        "        df = pd.DataFrame(all_products)\n",
        "        df = df[['Product_Name', 'MRP', 'Discount', 'Price', 'Ratings']]\n",
        "    else:\n",
        "        df = pd.DataFrame(columns=['Product_Name', 'MRP', 'Discount', 'Price', 'Ratings'])\n",
        "\n",
        "    df.to_csv('amazon_products.csv', index=False)\n",
        "    print()\n",
        "    print(\"Data saved to amazon_products.csv\")\n",
        "\n",
        "search = input(\"Enter what you want to search (e.g., laptop, phone): \")\n",
        "user_wanted_to_surf = int(input(\"Enter the number of pages to scrape: \"))\n",
        "\n",
        "scrape_amazon(search, user_wanted_to_surf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **If Error 503**\n",
        "    try to use proxy as amazon avoids their site data to being scraped"
      ],
      "metadata": {
        "id": "37wUczZ6n4j3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('amazon_products.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "wbs0s2kmn1Hp",
        "outputId": "1d5d2111-70d3-4824-be3c-adebe514e20e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_776dbeee-76d7-46e4-940d-9be21644cba4\", \"amazon_products.csv\", 40)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}